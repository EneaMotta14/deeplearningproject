{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Protein stability prediction\n",
    "\n",
    "In the project you will try to predict protein stability changes upon point mutations. \n",
    "We will use acuumulated data from experimental databases, i.e. the Megascale dataset. A current [paper](https://www.pnas.org/doi/10.1073/pnas.2314853121) has already preprocessed the dataset and created homology reduced data splits. We will reuse these. To do so, download the data folder from [here](https://polybox.ethz.ch/index.php/s/txvcb5jKy1A0TbY) and unzip it.  \n",
    "\n",
    "The data includes measurements of changes in the Gibbs free enrgy ($\\Delta \\Delta G $). \n",
    "This will be the value that you will have to predict for a given protein with a point mutation. \n",
    "As input data you can use the protein sequence or a protein embedding retreived from ESM, a state of the art protein model.  \n",
    "\n",
    "Here we will use the sequence as input. \n",
    "The model will predict the $\\Delta \\Delta G $ of point mutations in this sequence. To make training more efficient, the model should directly predict the values for all possible mutations at each position in the sequence. So the expected output is a sequence of $ L \\ (sequence \\ length) \\ x \\ 20 \\ (number \\ amino \\ acids)$. This will be shown in detail later.\n",
    "\n",
    "Below we provide you with a strcuture for the project that you can start with.  \n",
    "Edit the cells to your liking and add more code to create your final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.regression import PearsonCorrCoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "We are using the Megascale dataset. The train, validation and test sets are already predefined.\n",
    "We provide code to load the data and helper functions to encode the sequences to numerical vectors (one-hot encoding). You can use this code as a starting point, adjust it, or use your own data loading. \n",
    "\n",
    "Each sequence will be treated as one batch (easier to deal with different leghts this way). The class below returns a dictionary containing the one-hot encoded sequence of dimension $Lx20$, the target sequence of dimension $Lx20$, containing the $\\Delta \\Delta G $ values, and a mask of the same dimension which indicates with a 1 if an experimental value is available for that position. Only compute your loss on the positions where an experimental value is available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Encoding ##############\n",
    "\n",
    "aa_alphabet = 'ACDEFGHIKLMNPQRSTVWY' # amino acid alphabet\n",
    "aa_to_int = {aa: i for i, aa in enumerate(aa_alphabet)} # mapping from amino acid to number\n",
    "\n",
    "# function to one hot encode sequence\n",
    "def one_hot_encode(sequence):\n",
    "    # initialize a zero matrix of shape (len(sequence), len(amino_acids))\n",
    "    one_hot = torch.zeros(len(sequence), len(aa_alphabet))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        # set the column corresponding to the amino acid to 1\n",
    "        one_hot[i].scatter_(0, torch.tensor([aa_to_int[aa]]), 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# sequence data, comes already batched, so treat accordingly in dataloader (batch_size=1)\n",
    "class SequenceData(Dataset):\n",
    "    def __init__(self, csv_file, label_col=\"ddG_ML\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset. \n",
    "        input:\n",
    "            csv_file: path to the relevant data file, eg. \"/home/data/mega_train.csv\"\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
    "        self.label_col = label_col\n",
    "        # only have mutation rows\n",
    "        self.df = self.df[self.df.mut_type!=\"wt\"]\n",
    "        # process the mutation row\n",
    "        self.df[\"mutation_pos\"] = self.df[\"mut_type\"].apply(lambda x: int(x[1:-1])-1) # make position start at zero\n",
    "        self.df[\"mutation_to\"] = self.df[\"mut_type\"].apply(lambda x: aa_to_int[x[-1]]) # give numerical label to mutation\n",
    "\n",
    "        # group by wild type\n",
    "        self.df = self.df.groupby(\"WT_name\").agg(list)\n",
    "        # get wild type names\n",
    "        self.wt_names = self.df.index.values\n",
    "        # precompute one-hot encoding for faster training\n",
    "        self.encoded_seqs = {}\n",
    "        for wt_name in self.wt_names:\n",
    "            # get the correct row\n",
    "            mut_row = self.df.loc[wt_name]\n",
    "            seq = mut_row[\"wt_seq\"][0]\n",
    "            self.encoded_seqs[wt_name] = one_hot_encode(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the wild type name\n",
    "        wt_name = self.wt_names[idx]\n",
    "        # get the correct row\n",
    "        mut_row = self.df.loc[wt_name]\n",
    "        # get the wt sequence in one hot encoding\n",
    "        sequence_encoding = self.encoded_seqs[wt_name]\n",
    "\n",
    "        # create mask and target tensors\n",
    "        mask = torch.zeros((len(sequence_encoding),20)) # will be 1 where we have a measurement\n",
    "        target = torch.zeros((len(sequence_encoding),20)) # ddg values\n",
    "        # all mutations from df\n",
    "        positions = torch.tensor(mut_row[\"mutation_pos\"])\n",
    "        amino_acids = torch.tensor(mut_row[\"mutation_to\"])\n",
    "        # get the labels\n",
    "        labels = torch.tensor(mut_row[self.label_col])\n",
    "\n",
    "        for i in range(len(sequence_encoding)):\n",
    "            mask[i,amino_acids[positions==i]] = 1 # one where we have data\n",
    "            target[i,amino_acids[positions==i]] = labels[positions==i] # fill with ddG values\n",
    "        \n",
    "        # returns encoded sequence, mask and target sequence \n",
    "        return {\"sequence\": sequence_encoding[None,:,:].float()[0], \"mask\": mask, \"labels\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# usage\n",
    "dataset_train = SequenceData('project_data/mega_train.csv')\n",
    "dataset_val= SequenceData('project_data/mega_val.csv')\n",
    "dataset_test = SequenceData('project_data/mega_test.csv')\n",
    "\n",
    "# use batch_size=1 bc we treat each sequence as one batch\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Explore the data and try to understand what we are working with. How is the data structured? How is it distributed? What do the values mean? How is the data represented and how else could it be represented? \n",
    "Your approach depends on your understanding of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Set device to cuda if available #######################\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        # input parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # define model layers (LSTM)\n",
    "        self.LSTM = nn.LSTM(\n",
    "            self.input_dim,\n",
    "            self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        \n",
    "        # define fully connected layer\n",
    "        if self.bidirectional:\n",
    "            # For bidirectional LSTM, the output dimension is 2 * hidden_dim\n",
    "            self.fc1 = nn.Linear(2 * self.hidden_dim, self.output_dim)\n",
    "        else:\n",
    "            # For unidirectional LSTM, the output dimension is hidden_dim\n",
    "            self.fc1 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # Ensure input is 3-dimensional: (batch_size, sequence_length, input_dim)\n",
    "        if inp.dim() == 2:\n",
    "            inp = inp.unsqueeze(0)  # Add batch dimension if missing\n",
    "\n",
    "        # Move input to device\n",
    "        inp1 = inp.to(device)\n",
    "\n",
    "        # Initialize hidden state and cell state\n",
    "        if self.bidirectional:\n",
    "            # For bidirectional LSTM, h0 and c0 have shape (2 * num_layers, batch_size, hidden_dim)\n",
    "            h0 = torch.zeros(2 * self.num_layers, inp.size(0), self.hidden_dim).double().to(device)\n",
    "            c0 = torch.zeros(2 * self.num_layers, inp.size(0), self.hidden_dim).double().to(device)\n",
    "        else:\n",
    "            # For unidirectional LSTM, h0 and c0 have shape (num_layers, batch_size, hidden_dim)\n",
    "            h0 = torch.zeros(self.num_layers, inp.size(0), self.hidden_dim).double().to(device)\n",
    "            c0 = torch.zeros(self.num_layers, inp.size(0), self.hidden_dim).double().to(device)\n",
    "\n",
    "        # Get output for all time steps\n",
    "        out, (hn, cn) = self.LSTM(inp1, (h0, c0))\n",
    "\n",
    "        # Apply fully connected layer to all steps\n",
    "        output = self.fc1(out)  # Shape: (batch_size, sequence_length, output_dim)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with hidden_dim=5, learning_rate=0.1, num_layers=1, version=monodirectional_1_5_0.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 123\u001b[0m\n\u001b[1;32m    111\u001b[0m lit_model \u001b[38;5;241m=\u001b[39m SequenceModelLightning(\n\u001b[1;32m    112\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m    113\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     use_early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Define the trainer with custom version name\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m    124\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    125\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[1;32m    126\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[],  \u001b[38;5;66;03m# No early stopping for benchmarking\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Enable logging\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./lightning_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Root directory for logs\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion  \u001b[38;5;66;03m# Custom version name\u001b[39;00m\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m    133\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(lit_model, dataloader_train, dataloader_val)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/utilities/argparse.py:70\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mlist\u001b[39m(env_variables\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mitems()))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'version'"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate, num_layers, bidirectional, use_lr_scheduler, use_early_stopping):\n",
    "        super().__init__()\n",
    "        # Save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Initialize the model\n",
    "        self.model = MyLSTM(input_dim, hidden_dim, output_dim, num_layers, bidirectional).double()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define loss function\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def on_train_start(self):\n",
    "        \"\"\"Log hyperparameters after the logger is initialized.\"\"\"\n",
    "        if self.logger:\n",
    "            self.logger.log_hyperparams({\n",
    "                \"input_dim\": self.hparams.input_dim,\n",
    "                \"hidden_dim\": self.hparams.hidden_dim,\n",
    "                \"output_dim\": self.hparams.output_dim,\n",
    "                \"learning_rate\": self.hparams.learning_rate,\n",
    "                \"num_layers\": self.hparams.num_layers,\n",
    "                \"bidirectional\": self.hparams.bidirectional,\n",
    "                \"use_lr_scheduler\": self.hparams.use_lr_scheduler,\n",
    "                \"use_early_stopping\": self.hparams.use_early_stopping\n",
    "            })\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Extract input and target tensors\n",
    "        input_tensor = batch['sequence'].double()\n",
    "        target_tensor = batch['labels'].double()\n",
    "        mask = batch['mask'].double()\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Compute loss only for masked positions\n",
    "        loss = self.loss(output[mask == 1], target_tensor[mask == 1])\n",
    "\n",
    "        # Log training loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Extract input and target tensors\n",
    "        input_tensor = batch['sequence'].double()\n",
    "        target_tensor = batch['labels'].double()\n",
    "        mask = batch['mask'].double()\n",
    "\n",
    "        # Forward pass\n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        # Compute loss only for masked positions\n",
    "        loss = self.loss(output[mask == 1], target_tensor[mask == 1])\n",
    "\n",
    "        # Log validation loss\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer with the specified learning rate\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Define StepLR scheduler if enabled\n",
    "        if self.hparams.use_lr_scheduler:\n",
    "            scheduler = StepLR(\n",
    "                optimizer,\n",
    "                step_size=10,  # Reduce learning rate every 10 epochs\n",
    "                gamma=0.1      # Reduce learning rate by a factor of 0.1\n",
    "            )\n",
    "            return {\n",
    "                \"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": scheduler  # No need for \"monitor\" or \"interval\" with StepLR\n",
    "            }\n",
    "        else:\n",
    "            return optimizer\n",
    "\n",
    "########### Hyperparameters ###############\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "hidden_dims = [5, 25, 70, 170, 250]\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "num_layers_list = [1, 3, 5]\n",
    "\n",
    "# Final configuration\n",
    "final_config = {\n",
    "    \"hidden_dim\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"learning_rate\": 0.00130296\n",
    "}\n",
    "\n",
    "# Loop through all combinations\n",
    "for hidden_dim in hidden_dims:\n",
    "    for learning_rate in learning_rates:\n",
    "        for num_layers in num_layers_list:\n",
    "            # Create a custom version name for logging\n",
    "            version = f\"monodirectional_{num_layers}_{hidden_dim}_{learning_rate}\"\n",
    "            print(f\"Running with hidden_dim={hidden_dim}, learning_rate={learning_rate}, num_layers={num_layers}, version={version}\")\n",
    "\n",
    "            # Initialize the model with current hyperparameters\n",
    "            lit_model = SequenceModelLightning(\n",
    "                input_dim=20,\n",
    "                hidden_dim=hidden_dim,\n",
    "                output_dim=20,\n",
    "                learning_rate=learning_rate,\n",
    "                num_layers=num_layers,\n",
    "                bidirectional=True,\n",
    "                use_lr_scheduler=True,\n",
    "                use_early_stopping=False\n",
    "            )\n",
    "\n",
    "            # Define the trainer with custom version name\n",
    "            trainer = L.Trainer(\n",
    "                devices=1,\n",
    "                max_epochs=30,\n",
    "                callbacks=[],  # No early stopping for benchmarking\n",
    "                logger=True,  # Enable logging\n",
    "                default_root_dir=\"./lightning_logs\",  # Root directory for logs\n",
    "                version=version  # Custom version name\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            trainer.fit(lit_model, dataloader_train, dataloader_val)\n",
    "\n",
    "# Run the final configuration\n",
    "final_version = f\"monodirectional_{final_config['num_layers']}_{final_config['hidden_dim']}_{final_config['learning_rate']}\"\n",
    "print(f\"Running final configuration: hidden_dim={final_config['hidden_dim']}, learning_rate={final_config['learning_rate']}, num_layers={final_config['num_layers']}, version={final_version}\")\n",
    "\n",
    "# Initialize the model with final configuration\n",
    "lit_model = SequenceModelLightning(\n",
    "    input_dim=20,\n",
    "    hidden_dim=final_config[\"hidden_dim\"],\n",
    "    output_dim=20,\n",
    "    learning_rate=final_config[\"learning_rate\"],\n",
    "    num_layers=final_config[\"num_layers\"],\n",
    "    bidirectional=True,\n",
    "    use_lr_scheduler=True,\n",
    "    use_early_stopping=False\n",
    ")\n",
    "\n",
    "# Define the trainer with custom version name\n",
    "trainer = L.Trainer(\n",
    "    devices=1,\n",
    "    max_epochs=30,\n",
    "    callbacks=[],  # No early stopping for benchmarking\n",
    "    logger=True,  # Enable logging\n",
    "    default_root_dir=\"./lightning_logs\",  # Root directory for logs\n",
    "    version=final_version  # Custom version name\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(lit_model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cpu\")  # Force CPU\n",
    "\n",
    "# Move the model to the CPU\n",
    "lit_model.to(device)\n",
    "\n",
    "preds = []\n",
    "all_y = []\n",
    "\n",
    "for batch in dataloader_val:\n",
    "    # read from batch\n",
    "    x = batch[\"sequence\"].to(device)  # Move input tensor to CPU\n",
    "    mask = batch[\"mask\"].to(device)    # Move mask tensor to CPU\n",
    "    target = batch[\"labels\"].to(device)  # Move target tensor to CPU\n",
    "\n",
    "   # print('seq', x.shape)\n",
    "   # print('mask', mask.shape)\n",
    "   # print('target', target.shape)\n",
    "\n",
    "    # Convert input tensor to Double\n",
    "    x = x.double()\n",
    "\n",
    "    # predict\n",
    "    prediction = lit_model(x)\n",
    "\n",
    "    # Move tensors to CPU and convert to NumPy arrays\n",
    "    preds.append(prediction[mask == 1].flatten().cpu().detach().numpy())\n",
    "    all_y.append(target[mask == 1].flatten().cpu().detach().numpy())\n",
    "\n",
    "# Concatenate and plot\n",
    "preds = np.concatenate(preds)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "sns.regplot(x=preds, y=all_y)\n",
    "plt.xlabel(\"Predicted ddG\")\n",
    "plt.ylabel(\"Measured ddG\")\n",
    "\n",
    "# Get RMSE, Pearson, and Spearman correlation\n",
    "\n",
    "print(\"Pearson r:\", scipy.stats.pearsonr(preds, all_y))\n",
    "print(\"Spearman r:\", scipy.stats.spearmanr(preds, all_y))\n",
    "print(\"RMSE:\", skmetrics.mean_squared_error(all_y, preds,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cpu\")  # Force CPU\n",
    "\n",
    "# Move the model to the CPU\n",
    "lit_model.to(device)\n",
    "lit_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Initialize lists for predictions and targets\n",
    "preds = []\n",
    "all_y = []\n",
    "\n",
    "# Initialize a list to store RMSE for each position\n",
    "position_rmse = []\n",
    "\n",
    "# Validation loop\n",
    "for batch in dataloader_val:\n",
    "    # Read from batch\n",
    "    x = batch[\"sequence\"].to(device)  # Move input tensor to CPU\n",
    "    mask = batch[\"mask\"].to(device)    # Move mask tensor to CPU\n",
    "    target = batch[\"labels\"].to(device)  # Move target tensor to CPU\n",
    "\n",
    "    # Convert input tensor to Double\n",
    "    x = x.double()\n",
    "\n",
    "    # Predict\n",
    "    prediction = lit_model(x)  # Shape: (batch_size, sequence_length, 1)\n",
    "\n",
    "    # Move tensors to CPU and convert to NumPy arrays\n",
    "    preds.append(prediction[mask == 1].flatten().cpu().detach().numpy())\n",
    "    all_y.append(target[mask == 1].flatten().cpu().detach().numpy())\n",
    "\n",
    "    # Compute RMSE for each position in the sequence\n",
    "    for pos in range(prediction.shape[1]):  # Iterate over sequence positions\n",
    "        # Get predictions and targets for the current position\n",
    "        pred_pos = prediction[:, pos, :]  # Shape: (batch_size, 1)\n",
    "        target_pos = target[:, pos, :]   # Shape: (batch_size, 1)\n",
    "        mask_pos = mask[:, pos, :]       # Shape: (batch_size, 1)\n",
    "\n",
    "        # Compute RMSE only for masked positions\n",
    "        if mask_pos.sum() > 0:  # Check if there are any valid targets at this position\n",
    "            rmse_pos = torch.sqrt(torch.mean((pred_pos[mask_pos == 1] - target_pos[mask_pos == 1]) ** 2))\n",
    "            position_rmse.append((pos, rmse_pos.item()))\n",
    "\n",
    "# Concatenate predictions and targets\n",
    "preds = np.concatenate(preds)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "# Plot regression plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x=preds, y=all_y)\n",
    "plt.xlabel(\"Predicted ddG\")\n",
    "plt.ylabel(\"Measured ddG\")\n",
    "plt.title(\"Predicted vs Measured ddG\")\n",
    "plt.show()\n",
    "\n",
    "# Get RMSE, Pearson, and Spearman correlation\n",
    "print(\"Pearson r:\", scipy.stats.pearsonr(preds, all_y))\n",
    "print(\"Spearman r:\", scipy.stats.spearmanr(preds, all_y))\n",
    "print(\"RMSE:\", skmetrics.mean_squared_error(all_y, preds))\n",
    "\n",
    "# Convert RMSE values to a structured format\n",
    "position_rmse_dict = {}\n",
    "for pos, rmse in position_rmse:\n",
    "    if pos not in position_rmse_dict:\n",
    "        position_rmse_dict[pos] = []\n",
    "    position_rmse_dict[pos].append(rmse)\n",
    "\n",
    "# Compute average RMSE for each position\n",
    "avg_rmse_per_position = {pos: np.mean(rmses) for pos, rmses in position_rmse_dict.items()}\n",
    "\n",
    "# Sort positions and get corresponding average RMSE values\n",
    "sorted_positions = sorted(avg_rmse_per_position.keys())\n",
    "sorted_avg_rmse = [avg_rmse_per_position[pos] for pos in sorted_positions]\n",
    "\n",
    "# Plot the average RMSE for each position\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sorted_positions, sorted_avg_rmse, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel(\"Position in Sequence\")\n",
    "plt.ylabel(\"Average RMSE\")\n",
    "plt.title(\"Average RMSE per Position in Sequence\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Try to analyse and interpret your model and/or predictions in the context of the biological question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fa888489dcef296c36d3d3b759d2bbafdf14549bdfa862d5619a4427d05b08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
