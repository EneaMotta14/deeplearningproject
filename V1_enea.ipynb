{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Protein stability prediction\n",
    "\n",
    "In the project you will try to predict protein stability changes upon point mutations. \n",
    "We will use acuumulated data from experimental databases, i.e. the Megascale dataset. A current [paper](https://www.pnas.org/doi/10.1073/pnas.2314853121) has already preprocessed the dataset and created homology reduced data splits. We will reuse these. To do so, download the data folder from [here](https://polybox.ethz.ch/index.php/s/txvcb5jKy1A0TbY) and unzip it.  \n",
    "\n",
    "The data includes measurements of changes in the Gibbs free enrgy ($\\Delta \\Delta G $). \n",
    "This will be the value that you will have to predict for a given protein with a point mutation. \n",
    "As input data you can use the protein sequence or a protein embedding retreived from ESM, a state of the art protein model.  \n",
    "\n",
    "Here we will use the sequence as input. \n",
    "The model will predict the $\\Delta \\Delta G $ of point mutations in this sequence. To make training more efficient, the model should directly predict the values for all possible mutations at each position in the sequence. So the expected output is a sequence of $ L \\ (sequence \\ length) \\ x \\ 20 \\ (number \\ amino \\ acids)$. This will be shown in detail later.\n",
    "\n",
    "Below we provide you with a strcuture for the project that you can start with.  \n",
    "Edit the cells to your liking and add more code to create your final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/course/anaconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn.metrics as skmetrics\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import lightning as L\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.regression import PearsonCorrCoef"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "We are using the Megascale dataset. The train, validation and test sets are already predefined.\n",
    "We provide code to load the data and helper functions to encode the sequences to numerical vectors (one-hot encoding). You can use this code as a starting point, adjust it, or use your own data loading. \n",
    "\n",
    "Each sequence will be treated as one batch (easier to deal with different leghts this way). The class below returns a dictionary containing the one-hot encoded sequence of dimension $Lx20$, the target sequence of dimension $Lx20$, containing the $\\Delta \\Delta G $ values, and a mask of the same dimension which indicates with a 1 if an experimental value is available for that position. Only compute your loss on the positions where an experimental value is available. So compute your loss similar to this example:\n",
    "\n",
    "``` \n",
    "prediction = model(x)\n",
    "loss = loss(prediction[mask==1],labels[mask==1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa_alphabet = 'ACDEFGHIKLMNPQRSTVWY' # amino acid alphabet\n",
    "aa_to_int = {aa: i for i, aa in enumerate(aa_alphabet)} # mapping from amino acid to number\n",
    "\n",
    "# function to one hot encode sequence\n",
    "def one_hot_encode(sequence):\n",
    "    # initialize a zero matrix of shape (len(sequence), len(amino_acids))\n",
    "    one_hot = torch.zeros(len(sequence), len(aa_alphabet))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        # set the column corresponding to the amino acid to 1\n",
    "        one_hot[i].scatter_(0, torch.tensor([aa_to_int[aa]]), 1)\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "# sequence data, comes already batched, so treat accordingly in dataloader (batch_size=1)\n",
    "class SequenceData(Dataset):\n",
    "    def __init__(self, csv_file, label_col=\"ddG_ML\"):\n",
    "        \"\"\"\n",
    "        Initializes the dataset. \n",
    "        input:\n",
    "            csv_file: path to the relevant data file, eg. \"/home/data/mega_train.csv\"\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file, sep=\",\")\n",
    "        self.label_col = label_col\n",
    "        # only have mutation rows\n",
    "        self.df = self.df[self.df.mut_type!=\"wt\"]\n",
    "        # process the mutation row\n",
    "        self.df[\"mutation_pos\"] = self.df[\"mut_type\"].apply(lambda x: int(x[1:-1])-1) # make position start at zero\n",
    "        self.df[\"mutation_to\"] = self.df[\"mut_type\"].apply(lambda x: aa_to_int[x[-1]]) # give numerical label to mutation\n",
    "\n",
    "        # group by wild type\n",
    "        self.df = self.df.groupby(\"WT_name\").agg(list)\n",
    "        # get wild type names\n",
    "        self.wt_names = self.df.index.values\n",
    "        # precompute one-hot encoding for faster training\n",
    "        self.encoded_seqs = {}\n",
    "        for wt_name in self.wt_names:\n",
    "            # get the correct row\n",
    "            mut_row = self.df.loc[wt_name]\n",
    "            seq = mut_row[\"wt_seq\"][0]\n",
    "            self.encoded_seqs[wt_name] = one_hot_encode(seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get the wild type name\n",
    "        wt_name = self.wt_names[idx]\n",
    "        # get the correct row\n",
    "        mut_row = self.df.loc[wt_name]\n",
    "        # get the wt sequence in one hot encoding\n",
    "        sequence_encoding = self.encoded_seqs[wt_name]\n",
    "\n",
    "        # create mask and target tensors\n",
    "        mask = torch.zeros((len(sequence_encoding),20)) # will be 1 where we have a measurement\n",
    "        target = torch.zeros((len(sequence_encoding),20)) # ddg values\n",
    "        # all mutations from df\n",
    "        positions = torch.tensor(mut_row[\"mutation_pos\"])\n",
    "        amino_acids = torch.tensor(mut_row[\"mutation_to\"])\n",
    "        # get the labels\n",
    "        labels = torch.tensor(mut_row[self.label_col])\n",
    "\n",
    "        for i in range(len(sequence_encoding)):\n",
    "            mask[i,amino_acids[positions==i]] = 1 # one where we have data\n",
    "            target[i,amino_acids[positions==i]] = labels[positions==i] # fill with ddG values\n",
    "        \n",
    "        # returns encoded sequence, mask and target sequence \n",
    "        return {\"sequence\": sequence_encoding[None,:,:].float()[0], \"mask\": mask, \"labels\": target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usage\n",
    "dataset_train = SequenceData('project_data/mega_train.csv')\n",
    "dataset_val= SequenceData('project_data/mega_val.csv')\n",
    "dataset_test = SequenceData('project_data/mega_test.csv')\n",
    "\n",
    "# use batch_size=1 bc we treat each sequence as one batch\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=1, shuffle=False)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Explore the data and try to understand what we are working with. How is the data structured? How is it distributed? What do the values mean? How is the data represented and how else could it be represented? \n",
    "Your approach depends on your understanding of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63, 20])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_train[0]['sequence'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x71f9ab549e50>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model architecture and training\n",
    "\n",
    "Now it's your turn. Create a model trained on the given sequences.  \n",
    "Be aware that this is not a classification task, but a regression task. You want to predict continuous numbers as close to the measured $\\Delta \\Delta G $ value as possible.\n",
    "You will need to adjust your architecture and loss accordingly.\n",
    "\n",
    "Train the model with the predefined dataloaders and try to improve it. \n",
    "Only test on the test set at the very end, when you have finished fine-tuning you model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(LSTMPredictor, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)  # Output a single value (ddG)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # lstm_out: (batch_size, seq_len, hidden_dim)\n",
    "        predictions = self.fc(lstm_out)  # predictions: (batch_size, seq_len, 1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyLSTM,self).__init__()\n",
    "        \n",
    "        # input parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "\n",
    "        # define model layers (LSTM), pseudocode:\n",
    "        self.LSTM = nn.LSTM(self.input_dim, self.hidden_dim, num_layers = 1, batch_first = True)\n",
    "        # define full connected layer\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "\n",
    "    def forward(self,inp):\n",
    "        inp1 = inp.to(device)\n",
    "        c0 = torch.zeros(1, inp.size(0), self.hidden_dim).double().to(device)\n",
    "        h0 = torch.zeros(1, inp.size(0), self.hidden_dim).double().to(device)\n",
    "        #print(\"Input shape:\", inp.shape)\n",
    "        #print('inp:', inp1)\n",
    "        #print('c0', c0)\n",
    "        #print('h0', h0)\n",
    "        #get output for all time steps\n",
    "        out, (hn, cn) = self.LSTM(inp1, (h0, c0))\n",
    "\n",
    "        #apply fc to all steps\n",
    "        output= self.fc1(out)\n",
    "     #   print(\"output shape:\", output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "####lightning module to train the sequence model\n",
    "class SequenceModelLightning(L.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.model = MyLSTM(input_dim, hidden_dim, output_dim).double()\n",
    "        self.learning_rate = learning_rate\n",
    "        # define loss function here, pseudocode:\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tensor = batch['sequence'].double()\n",
    "        target_tensor = batch['labels'].double()\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "        loss = self.loss(output, target_tensor)\n",
    "     #   loss = self.loss(output.view(-1, output.shape[2]),target_tensor.view(-1).long())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "     #   print('training good')\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_tensor = batch['sequence'].double()\n",
    "        target_tensor = batch['labels'].double()\n",
    "     #   print(\"target shape:\", target_tensor.shape)\n",
    "     #   print('batchsize', input_tensor[0].shape)\n",
    "        output = self.model(input_tensor)\n",
    "     #   print(\"output shape:\", output.shape)\n",
    "        loss = self.loss(output, target_tensor)\n",
    "     #   loss = self.loss(output.view(-1, output.shape[2]),target_tensor[0].view(-1).long())\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "     #   print('validation good')\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # define optimizer here\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | MyLSTM           | 1.5 K  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 239/239 [00:01<00:00, 189.84it/s, v_num=28, train_loss=-1.15e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 435.41it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 299.41it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 264.95it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 281.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 262.53it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 267.03it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 275.99it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 270.69it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 275.53it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 273.89it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 273.63it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 272.95it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 265.86it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 259.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 262.29it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 258.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 258.98it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 253.55it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 254.55it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 255.75it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 257.06it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 255.83it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 255.85it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 257.33it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 258.23it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 256.22it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 255.36it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 254.54it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 254.52it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 252.36it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 251.81it/s]\n",
      "Epoch 1: 100%|█| 239/239 [00:01<00:00, 191.65it/s, v_num=28, train_loss=-2.74e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 535.06it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 296.82it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 266.59it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 284.43it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 276.12it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 275.55it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 266.37it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 238.21it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 244.06it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 247.59it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 249.94it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 252.80it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 249.06it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 243.54it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 245.61it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 243.80it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 245.56it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 241.52it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 242.02it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 243.69it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 245.16it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 244.15it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 243.15it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 244.37it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 245.98it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 245.95it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 244.73it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 244.20it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 244.88it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 243.70it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 243.36it/s]\n",
      "Epoch 2: 100%|█| 239/239 [00:01<00:00, 193.51it/s, v_num=28, train_loss=-2.47e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 426.29it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 280.07it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 258.21it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 277.00it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 266.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 271.02it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 279.66it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 276.08it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 284.58it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 288.43it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 290.64it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 290.63it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 283.22it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 280.43it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 280.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 274.37it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 273.87it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 269.85it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 267.93it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 267.71it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 268.41it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 269.67it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 268.58it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 268.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 269.37it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 269.96it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 270.38it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 268.17it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 267.46it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 267.01it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 267.02it/s]\n",
      "Epoch 3: 100%|█| 239/239 [00:01<00:00, 195.00it/s, v_num=28, train_loss=-9.28e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 415.61it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 284.01it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 260.60it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 278.45it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 265.22it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 262.14it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 267.28it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 261.75it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 269.03it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 271.72it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 273.59it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 275.50it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 269.62it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 265.33it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 267.17it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 262.16it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 261.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 258.14it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 257.83it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 258.37it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 260.00it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 261.32it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 260.47it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 260.49it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 260.97it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 260.97it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 259.72it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 257.63it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 257.31it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 257.03it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 256.69it/s]\n",
      "Epoch 4: 100%|█| 239/239 [00:01<00:00, 190.24it/s, v_num=28, train_loss=-1.18e+4\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 429.57it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 286.35it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 265.07it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 283.43it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 261.75it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 263.72it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 270.27it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 264.83it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 270.15it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 271.23it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 273.41it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 275.63it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 270.61it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 266.18it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 267.79it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 263.10it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 263.82it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 258.28it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 258.66it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 259.99it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 261.52it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 260.74it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 260.78it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 261.78it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 262.86it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 262.60it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 261.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 260.06it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 260.09it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 259.55it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 258.12it/s]\n",
      "Epoch 5: 100%|█| 239/239 [00:01<00:00, 189.17it/s, v_num=28, train_loss=-7.92e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 491.54it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 320.21it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 264.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 271.06it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 263.91it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 268.45it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 267.67it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 255.67it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 261.57it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 266.83it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 271.07it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 269.46it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 261.88it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 259.87it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 263.37it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 257.00it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 256.38it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 253.99it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 254.33it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 253.49it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 253.72it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 254.96it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 256.15it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 256.56it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 256.79it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 256.89it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 256.86it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 254.56it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 254.47it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 254.28it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 254.55it/s]\n",
      "Epoch 6: 100%|█| 239/239 [00:01<00:00, 188.55it/s, v_num=28, train_loss=-4.28e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 487.65it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 310.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 265.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 266.20it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 252.36it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 256.13it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 263.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 255.35it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 258.98it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 262.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 267.76it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 269.63it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 260.41it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 258.04it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 261.31it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 257.38it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 255.16it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 251.60it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 252.38it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 252.73it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 253.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 253.90it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 254.57it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 255.45it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 255.87it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 255.48it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 255.75it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 254.89it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 253.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 253.14it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 253.39it/s]\n",
      "Epoch 7: 100%|█| 239/239 [00:01<00:00, 187.58it/s, v_num=28, train_loss=-1.62e+4\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 498.73it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 310.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 261.47it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 273.81it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 265.40it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 267.76it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 269.60it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 260.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 266.78it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 272.59it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 277.06it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 275.17it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 268.28it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 266.11it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 269.44it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 265.46it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 264.72it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 262.08it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 263.42it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 261.61it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 262.26it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 263.35it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 264.52it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 264.58it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 265.11it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 264.38it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 264.38it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 263.03it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 261.99it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 260.96it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 260.63it/s]\n",
      "Epoch 8: 100%|█| 239/239 [00:01<00:00, 187.50it/s, v_num=28, train_loss=-7.51e+3\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 529.65it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 302.82it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 254.97it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 264.20it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 255.33it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 257.14it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 260.60it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 253.99it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 260.12it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 263.59it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 264.25it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 263.41it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 258.54it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 257.66it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 260.35it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 255.40it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 255.19it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 253.18it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 251.84it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 252.24it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 253.34it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 254.63it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 256.00it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 256.19it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 256.48it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 256.52it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 256.29it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 254.13it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 254.32it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 254.04it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 254.18it/s]\n",
      "Epoch 9: 100%|█| 239/239 [00:01<00:00, 191.97it/s, v_num=28, train_loss=-2.9e+4,\n",
      "\u001b[Aidation: |                                             | 0/? [00:00<?, ?it/s]\n",
      "\u001b[Aidation:   0%|                                        | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   0%|                           | 0/31 [00:00<?, ?it/s]\n",
      "\u001b[Aidation DataLoader 0:   3%|▌                 | 1/31 [00:00<00:00, 434.10it/s]\n",
      "\u001b[Aidation DataLoader 0:   6%|█▏                | 2/31 [00:00<00:00, 276.83it/s]\n",
      "\u001b[Aidation DataLoader 0:  10%|█▋                | 3/31 [00:00<00:00, 248.52it/s]\n",
      "\u001b[Aidation DataLoader 0:  13%|██▎               | 4/31 [00:00<00:00, 260.19it/s]\n",
      "\u001b[Aidation DataLoader 0:  16%|██▉               | 5/31 [00:00<00:00, 245.68it/s]\n",
      "\u001b[Aidation DataLoader 0:  19%|███▍              | 6/31 [00:00<00:00, 249.90it/s]\n",
      "\u001b[Aidation DataLoader 0:  23%|████              | 7/31 [00:00<00:00, 256.17it/s]\n",
      "\u001b[Aidation DataLoader 0:  26%|████▋             | 8/31 [00:00<00:00, 249.74it/s]\n",
      "\u001b[Aidation DataLoader 0:  29%|█████▏            | 9/31 [00:00<00:00, 254.11it/s]\n",
      "\u001b[Aidation DataLoader 0:  32%|█████▍           | 10/31 [00:00<00:00, 259.87it/s]\n",
      "\u001b[Aidation DataLoader 0:  35%|██████           | 11/31 [00:00<00:00, 264.78it/s]\n",
      "\u001b[Aidation DataLoader 0:  39%|██████▌          | 12/31 [00:00<00:00, 267.04it/s]\n",
      "\u001b[Aidation DataLoader 0:  42%|███████▏         | 13/31 [00:00<00:00, 256.67it/s]\n",
      "\u001b[Aidation DataLoader 0:  45%|███████▋         | 14/31 [00:00<00:00, 254.55it/s]\n",
      "\u001b[Aidation DataLoader 0:  48%|████████▏        | 15/31 [00:00<00:00, 257.96it/s]\n",
      "\u001b[Aidation DataLoader 0:  52%|████████▊        | 16/31 [00:00<00:00, 255.15it/s]\n",
      "\u001b[Aidation DataLoader 0:  55%|█████████▎       | 17/31 [00:00<00:00, 252.34it/s]\n",
      "\u001b[Aidation DataLoader 0:  58%|█████████▊       | 18/31 [00:00<00:00, 248.75it/s]\n",
      "\u001b[Aidation DataLoader 0:  61%|██████████▍      | 19/31 [00:00<00:00, 249.51it/s]\n",
      "\u001b[Aidation DataLoader 0:  65%|██████████▉      | 20/31 [00:00<00:00, 250.57it/s]\n",
      "\u001b[Aidation DataLoader 0:  68%|███████████▌     | 21/31 [00:00<00:00, 249.93it/s]\n",
      "\u001b[Aidation DataLoader 0:  71%|████████████     | 22/31 [00:00<00:00, 250.91it/s]\n",
      "\u001b[Aidation DataLoader 0:  74%|████████████▌    | 23/31 [00:00<00:00, 252.58it/s]\n",
      "\u001b[Aidation DataLoader 0:  77%|█████████████▏   | 24/31 [00:00<00:00, 253.94it/s]\n",
      "\u001b[Aidation DataLoader 0:  81%|█████████████▋   | 25/31 [00:00<00:00, 254.58it/s]\n",
      "\u001b[Aidation DataLoader 0:  84%|██████████████▎  | 26/31 [00:00<00:00, 255.10it/s]\n",
      "\u001b[Aidation DataLoader 0:  87%|██████████████▊  | 27/31 [00:00<00:00, 255.10it/s]\n",
      "\u001b[Aidation DataLoader 0:  90%|███████████████▎ | 28/31 [00:00<00:00, 254.05it/s]\n",
      "\u001b[Aidation DataLoader 0:  94%|███████████████▉ | 29/31 [00:00<00:00, 254.19it/s]\n",
      "\u001b[Aidation DataLoader 0:  97%|████████████████▍| 30/31 [00:00<00:00, 254.07it/s]\n",
      "\u001b[Aidation DataLoader 0: 100%|█████████████████| 31/31 [00:00<00:00, 253.95it/s]\n",
      "Epoch 9: 100%|█| 239/239 [00:01<00:00, 174.10it/s, v_num=28, train_loss=-2.9e+4,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█| 239/239 [00:01<00:00, 173.82it/s, v_num=28, train_loss=-2.9e+4,\n"
     ]
    }
   ],
   "source": [
    "lit_model = SequenceModelLightning(input_dim = 20,\n",
    "                                  hidden_dim = 10,\n",
    "                                  output_dim = 20,\n",
    "                                  learning_rate = .001)\n",
    "\n",
    "# define the trainer\n",
    "trainer = L.Trainer(devices = 1, \n",
    "                    max_epochs = 10)\n",
    "\n",
    "# learn the weights of the model\n",
    "trainer.fit(lit_model, dataloader_train, dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "input_dim = 20  # One-hot encoding size\n",
    "hidden_dim = 64  # LSTM hidden dimension\n",
    "num_layers = 1  # Number of LSTM layers\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for batch in dataloader_val:\n",
    "        sequence = batch[\"sequence\"]\n",
    "        mask = batch[\"mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        predictions = model(sequence)\n",
    "        loss = masked_mse_loss(predictions, labels, mask)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Validation Loss: {total_loss / len(dataloader_val):.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and visualization\n",
    "\n",
    "To get a good feeling of how the model is performing and to compare with literature, compute the Pearson and Spearman correlations.\n",
    "You can also plot the predictions in a scatterplot. We have added some code for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not the same dtype, found input tensor with Float and hidden tensor with Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m lit_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m## adjust to work with your model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# predict\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     13\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(prediction[mask\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;66;03m# flatten to create one dimensional vector from 2D sequence\u001b[39;00m\n\u001b[1;32m     14\u001b[0m all_y\u001b[38;5;241m.\u001b[39mappend(target[mask\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;66;03m# flatten to create one dimensional vector from 2D sequence\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mSequenceModelLightning.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[34], line 25\u001b[0m, in \u001b[0;36mMyLSTM.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     19\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, inp\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(\"Input shape:\", inp.shape)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print('inp:', inp1)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print('c0', c0)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#print('h0', h0)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#get output for all time steps\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m out, (hn, cn) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLSTM(inp1, (h0, c0))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#apply fc to all steps\u001b[39;00m\n\u001b[1;32m     28\u001b[0m output\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and hidden tensors are not the same dtype, found input tensor with Float and hidden tensor with Double"
     ]
    }
   ],
   "source": [
    "preds =[]\n",
    "all_y = []\n",
    "\n",
    "for batch in dataloader_val:\n",
    "    # read from batch\n",
    "    x = batch[\"sequence\"].to(device)\n",
    "    mask = batch[\"mask\"].to(device)\n",
    "    target = batch[\"labels\"].to(device)\n",
    "    model = lit_model.to(device)\n",
    "    ## adjust to work with your model\n",
    "    # predict\n",
    "    prediction = model(x)\n",
    "    preds.append(prediction[mask==1].flatten().detach().numpy()) # flatten to create one dimensional vector from 2D sequence\n",
    "    all_y.append(target[mask==1].flatten().detach().numpy()) # flatten to create one dimensional vector from 2D sequence\n",
    "\n",
    "# concatenate and plot\n",
    "preds= np.concatenate(preds)\n",
    "all_y = np.concatenate(all_y)\n",
    "\n",
    "sns.regplot(x=preds,y=all_y)\n",
    "plt.xlabel(\"Predicted ddG\")\n",
    "plt.ylabel(\"Measured ddG\")\n",
    "\n",
    "# get RMSE, Pearson and Spearman correlation \n",
    "print(\"RMSE:\", skmetrics.mean_squared_error(all_y, preds, squared=\"False\"))\n",
    "print(\"Pearson r:\", scipy.stats.pearsonr(preds, all_y))\n",
    "print(\"Spearman r:\", scipy.stats.spearmanr(preds, all_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "Try to analyse and interpret your model and/or predictions in the context of the biological question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fa888489dcef296c36d3d3b759d2bbafdf14549bdfa862d5619a4427d05b08f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
